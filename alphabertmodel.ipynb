{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train_data_final_exported and test_data_final_exported from a pickle file\n",
    "\n",
    "train_data_final_exported,_= pickle.load(open('./data/data_for_model.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c1cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional code for TensorFlow stability\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a828198",
   "metadata": {},
   "source": [
    "# Domain adaptation of AlephBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18503bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is based on the HuggingFace instruction for domain adaptation of a pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7fc486",
   "metadata": {},
   "source": [
    "## Loading the pretrained data and prepairing for domain adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696b238",
   "metadata": {},
   "source": [
    "This section I use a large non labled data of hospital notes regarrding other patients to adapt the pretrainied model weights to be closer to the \"medical domain\" thus improve its performace when finetuning.  \n",
    "*This however does not add additional words to the embedding which is only possible when training form scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretraining data that was prepaired before\n",
    "pretraing_data= pickle.load(open('./data/pretraining_data.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset,Features, Value, ClassLabel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b431bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pretraining_data to a Dataset object\n",
    "pretraing_data=Dataset.from_pandas(pretraing_data,Features({'text': Value('string')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification,TFAutoModelForMaskedLM\n",
    "model_checkpoint = \"onlplab/alephbert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34297fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e454d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f31e55c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenized_datasets = pretraing_data.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91121314",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group texts into chunks for language modeling\n",
    "\n",
    "def group_texts(examples):\n",
    "    chunk_size = 64\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d5fd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=4)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8809f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a804f60",
   "metadata": {},
   "source": [
    "## Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "train_size = 400000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets.train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = downsampled_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = downsampled_dataset[\"test\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a53831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "callbacks = tf.keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a15d88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=1e-5,\n",
    "    num_warmup_steps=10,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16 - another stability and calability optional code\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Use perplexity to evaluate the adaptation of the model to our data\n",
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb2b32b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset,validation_data=tf_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afe2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model is saved\n",
    "model.save_pretrained('./models/aleph_bert_med')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a758cd1",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "model_checkpoint = \"onlplab/alephbert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c165c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train=train_data_final_exported[['row_text','clf']].copy()\n",
    "train,val=train_test_split(train,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Value, ClassLabel,load_dataset,Dataset\n",
    "features_load = Features({'row_text': Value('string'), 'clf': ClassLabel(num_classes=2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4644e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=Dataset.from_pandas(train.reset_index(drop=True), features=features_load)\n",
    "val_df=Dataset.from_pandas(val.reset_index(drop=True),features=features_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['row_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a61276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_train = train_df.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cef3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_val = val_df.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdee8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2242fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_train.to_tf_dataset(\n",
    "    columns=[\"input_ids\",'token_type_ids',\"attention_mask\"],\n",
    "    label_cols=[\"clf\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "tf_val_dataset = tokenized_val.to_tf_dataset(\n",
    "    columns=[\"input_ids\",'token_type_ids',\"attention_mask\"],\n",
    "    label_cols=[\"clf\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae54494",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = './models/aleph_bert_med'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61813eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "import random\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "min_loss=math.inf\n",
    "for i in tqdm(range(50)):\n",
    "    random.seed(i)\n",
    "    num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "    lr_scheduler = PolynomialDecay(\n",
    "        initial_learning_rate=1e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    "    )\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "    opt = Adam(learning_rate=lr_scheduler)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    model.fit(tf_train_dataset, validation_data=tf_val_dataset,batch_size=batch_size, \n",
    "          epochs=num_epochs, shuffle=True,verbose=0)\n",
    "    new_loss=model.evaluate(tf_val_dataset)\n",
    "    if new_loss < min_loss:\n",
    "        model.save_pretrained('./models/aleph_bert_finetuned')\n",
    "        min_loss=new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b426cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
