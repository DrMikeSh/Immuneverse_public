{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a87436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb87f3",
   "metadata": {},
   "source": [
    "Prepare the information regarding the patients examine before for requesting additional hospital data after the initial follow-up.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c0636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_to_drug=pd.read_csv(r'./data/Id_to_drug.csv', parse_dates=['min_date','max_added'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dates=id_to_drug[['PatNum','max_added']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be608726",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dates['cutoff_date']='2022-10-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dates[['PatNum','max_added','cutoff_date']].to_csv('to_extract_additional_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ea98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data was sent to the hospital to retrieve all notes for the relevant patients between the max_added and cutoff_date\n",
    "# The file returned was named - after_treatment_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad94eae",
   "metadata": {},
   "source": [
    "# Get notes potentially containing IrAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_treatment_data=pd.read_csv('./data/after_treatment_data.csv', low_memory=False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc020710",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_treatment_data=after_treatment_data.dropna(subset=['Description_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9476e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(text_file):\n",
    "    # Get all words in the database \n",
    "    import re,string\n",
    "    all_words = set()\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    for row in text_file.itertuples():\n",
    "        try:\n",
    "            description_text = regex.sub('', row.Description_Text)\n",
    "        except:\n",
    "            pass\n",
    "        all_words.update(description_text.split())\n",
    "        \n",
    "    #  Get similar words to IrAEs in Hebrew and English   \n",
    "    from thefuzz import process,fuzz\n",
    "    itis_words={'קוליטיס':'colitis', 'colitis':'colitis','דרמטיטיס':'dermatitis', 'dermatitis':'dermatitis', \n",
    "           'הפטיטיס':'hepatitis','hepatitis':'hepatitis', 'מיאסטניה':'myasthenia','myasthenia':'myasthenia',\n",
    "           'מיוקרדיטיס':'myocarditis','myocarditis':'myocarditis','פנאומוניטיס':'pneumonitis','pneumonitis':'pneumonitis',\n",
    "           'תירואידיטיס':'thyroiditis','thyroiditis':'thyroiditis'}\n",
    "    from collections import defaultdict\n",
    "    similar_words_dict = defaultdict(list)\n",
    "    for key in itis_words.keys():\n",
    "        similar_words = process.extract(key, all_words, limit=500, scorer=fuzz.token_set_ratio)\n",
    "        similar_words_final = [res[0] for res in similar_words if res[1] >= 80]\n",
    "        similar_words_dict[itis_words[key]] += similar_words_final\n",
    "    return similar_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928fe037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_for_word(search_words, df,itis_category, n_words=5):\n",
    "    import re,string\n",
    "    result = []\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        description_text = regex.sub('', row.Description_Text)\n",
    "        description_text_list = description_text.split()\n",
    "        for i, word in enumerate(description_text_list):\n",
    "            if word in search_words:\n",
    "                start_index = max(0, i - n_words)\n",
    "                end_index = min(i + n_words + 1, len(description_text_list))\n",
    "                X = description_text_list[start_index:end_index]\n",
    "                row_text = \" \".join(X)\n",
    "                row_context = ' '.join(description_text_list[max(0, i - n_words * 2):min(i + n_words * 2 + 1, len(description_text_list))])\n",
    "\n",
    "                result.append([row.PatNum, row.Entry_Date, row.Description_Text, X, row_text, row_context, itis_category, word])\n",
    "\n",
    "    result = pd.DataFrame(result, columns=['PatNum', 'Entry_Date', 'Description_Text','X', 'row_text', 'row_context', 'itis_category','word'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf8576b",
   "metadata": {},
   "source": [
    "# Functions for running the prediction using both pretrained AlphaBert and FastText models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03084344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_joined_prediction(test_data):\n",
    "    import re,string\n",
    "    import math\n",
    "    import tensorflow as tf\n",
    "    from transformers import TFAutoModelForSequenceClassification\n",
    "    from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "    import random\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from transformers import AutoTokenizer, TFAutoModelForSequenceClassification,DataCollatorWithPadding\n",
    "    from datasets import Features, Value, ClassLabel,load_dataset,Dataset\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Input,Bidirectional,Dropout,Multiply, LSTM\n",
    "    callbacks = tf.keras.callbacks\n",
    "\n",
    "    #   Helper funtions\n",
    "    def logit_to_prob(logit):\n",
    "        odds= math.exp(logit)\n",
    "        return odds/(1+odds)\n",
    "    \n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example['row_text'])\n",
    "\n",
    "    #  Alphabert model\n",
    "    finetuned_model = './models/aleph_bert_finetuned'\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    general_model_checkpoint = \"onlplab/alephbert-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(general_model_checkpoint)\n",
    "    features_load = Features({'row_text': Value('string')})\n",
    "    test_df=Dataset.from_pandas(test_data[['row_text']].reset_index(drop=True),features=features_load)\n",
    "    tokenized_test = test_df.map(tokenize_function, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "    tf_test_dataset = tokenized_test.to_tf_dataset(\n",
    "    columns=[\"input_ids\",'token_type_ids',\"attention_mask\"], shuffle=False, collate_fn=data_collator,batch_size=32,)\n",
    "\n",
    "    batch_size = 32\n",
    "    num_epochs = 5\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(finetuned_model, num_labels=2)\n",
    "    lr_scheduler = PolynomialDecay(\n",
    "        initial_learning_rate=1e-5, end_learning_rate=0.0, decay_steps=300\n",
    "    )\n",
    "    opt = Adam(learning_rate=lr_scheduler)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    preds = model.predict(tf_test_dataset)[\"logits\"]\n",
    "    y_prob_bert=[logit_to_prob(x) for x in preds[:,1]]\n",
    "    \n",
    "    ##  FastText model\n",
    "    from gensim.models import FastText\n",
    "    fasttext_model=FastText.load('./models/medical_fast_text.model')\n",
    "    X_test_fasttext=[]\n",
    "    for x in test_data['X']:\n",
    "        row=[]\n",
    "        for word in x:\n",
    "            row.append(fasttext_model.wv[word])\n",
    "        while len(row)<11:\n",
    "            row.append([0]*300)\n",
    "        X_test_fasttext.append(row)\n",
    "    X_test_fasttext=np.stack(X_test_fasttext)\n",
    "\n",
    "    # The model LSTM architecture\n",
    "    inputA = Input(shape=(X_test_fasttext.shape[1],X_test_fasttext.shape[2],))\n",
    "    x = Bidirectional(LSTM(50, return_sequences=False))(inputA)\n",
    "    x=Dense(10, activation='relu')(x)\n",
    "    prefinal=Dense(5, activation='relu')(x)\n",
    "    final = Dense(1, activation='sigmoid')(prefinal)\n",
    "    model = tf.keras.Model(inputs=[inputA], outputs=final)\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.load_weights('./models/fast_text_best.hdf5')\n",
    "    fast_text_results=model.predict(X_test_fasttext)\n",
    "    mean_prob=[(x+y)/2 for x,y in zip(fast_text_results,y_prob_bert)]\n",
    "    y_pred_joined=[1 if y>=0.5 else 0 for y in mean_prob]\n",
    "    return y_pred_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d82451",
   "metadata": {},
   "source": [
    "# Run all the process and time it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cd72e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# get the similar word to IrAEs\n",
    "similar_words_dict=get_words(after_treatment_data)\n",
    "\n",
    "# get the phrases where these words are present in the text. \n",
    "from joblib import Parallel, delayed\n",
    "def process_key(key):\n",
    "    return get_row_for_word(similar_words_dict[key], after_treatment_data, key, n_words=5)\n",
    "final_rows = pd.concat(Parallel(n_jobs=-1)(delayed(process_key)(key) for key in similar_words_dict))\n",
    "final_data_for_prediction=final_rows[['PatNum', 'Entry_Date','row_text','X','itis_category']].copy()\n",
    "results=run_the_joined_prediction(final_data_for_prediction)\n",
    "final_data_for_prediction['clf']=results\n",
    "\n",
    "# IrAEs are considered positive if they are reported positively more than one time in the EHR. \n",
    "final_data_for_prediction=final_data_for_prediction.merge(id_to_drug[['PatNum','drug']], on=['PatNum'], how='left')\n",
    "final_data_for_prediction_pos=final_data_for_prediction.loc[final_data_for_prediction.clf==1].copy()\n",
    "grouped_pos = final_data_for_prediction_pos.groupby(['drug', 'itis_category', 'PatNum'])['Entry_Date'].count().reset_index(name='DateCount')\n",
    "grouped_pos=grouped_pos.loc[grouped_pos.DateCount>1].copy()\n",
    "print(grouped_pos.groupby(['drug', 'itis_category'])['PatNum'].count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
